# mlops-housing-pipeline
To run, in the root directory, run the command "docker-compose up --build" 

1. FastAPI â€” The ML Model as a Web Service
	â€¢	What it is: A Python web framework used to expose your ML model as a REST API.
	â€¢	What it does in your setup:
	â€¢	Loads your trained ML model.
	â€¢	Provides an endpoint like /predict that accepts input (via JSON) and returns predictions.
	â€¢	Exposes a /metrics endpoint (via prometheus_client) to show internal metrics like:
	â€¢	Request count
	â€¢	Latency
	â€¢	Error count

âœ… Think of FastAPI as the face of your ML model, serving predictions to users or other systems.

â¸»

ğŸ”· 2. Prometheus â€” The Metrics Collector
	â€¢	What it is: An open-source system for monitoring and alerting, built for time-series data.
	â€¢	What it does in your setup:
	â€¢	Periodically scrapes your FastAPIâ€™s /metrics endpoint.
	â€¢	Collects data like:
	â€¢	How many times the model is being called
	â€¢	How long predictions take
	â€¢	If any errors are happening

âœ… Think of Prometheus as the data-gatherer that checks the health and usage of your ML API.

â¸»

ğŸ”· 3. Grafana â€” The Visualization Layer
	â€¢	What it is: An open-source dashboarding tool.
	â€¢	What it does in your setup:
	â€¢	Connects to Prometheus as a data source.
	â€¢	Displays live dashboards for metrics like:
	â€¢	Number of API hits over time
	â€¢	Average response time
	â€¢	Status codes
	â€¢	Any model-related metrics you track
	â€¢	Helps you visualize system performance and detect anomalies or drift.
	http://localhost:3000
	â€¢	Username: admin
	â€¢	Password: admin

	After login, you can:
	â€¢	Add Prometheus as a Data Source:
	â€¢	Go to âš™ï¸ â€œSettingsâ€ > â€œData Sourcesâ€
	â€¢	Click â€œAdd data sourceâ€
	â€¢	Select Prometheus
	â€¢	Set URL as http://prometheus:9090
	â€¢	Click â€œSave & Testâ€
	â€¢	Import Dashboard:
	â€¢	Use predefined JSON dashboard templates
	â€¢	Or create panels showing custom metrics (e.g., from your FastAPI app)

First login will prompt you to change the password â€” you can set a new one or keep it as is for testing.

âœ… Think of Grafana as the monitoring dashboard â€” it shows you everything happening under the hood of your model.

â¸»

ğŸ” Putting It All Together

Hereâ€™s a step-by-step flow:
	1.	ğŸ”„ User/API calls FastAPI endpoint â†’ /predict
	2.	âš™ï¸ FastAPI handles the request and returns a prediction.
	3.	ğŸ“Š FastAPI updates internal metrics (via prometheus_client).
	4.	â±ï¸ Prometheus scrapes /metrics every 15s (or configured interval).
	5.	ğŸ“ˆ Grafana pulls metrics from Prometheus and displays them in real-time dashboards.



ğŸ³ What is Docker?

Docker is a tool that lets you package your code + all dependencies + environment into a single portable unit called a container.

Think of it like creating a mini, isolated Linux machine that runs only your ML app â€” no matter which system youâ€™re on.

â¸»

ğŸ’¡ In Your MLOps Project, Docker Is Doing the Following:

ğŸ”¹ 1. Packaging Your FastAPI App
	â€¢	It takes your FastAPI-based ML service (main.py + model + requirements.txt)
	â€¢	Bundles it into a container using a Dockerfile
	â€¢	This container can now run on any system â€” Linux, Mac, Windows, local machine, or cloud â€” without â€œit works on my machineâ€ issues

â¸»

ğŸ”¹ 2. Running Prometheus and Grafana
	â€¢	In your docker-compose.yml, youâ€™re also pulling Docker images for:
	â€¢	Prometheus: from prom/prometheus
	â€¢	Grafana: from grafana/grafana
	â€¢	These images are pre-configured services that spin up immediately â€” no need to install them manually

â¸»

ğŸ”¹ 3. Connecting All Services Together
	â€¢	Docker Compose sets up a mini local cluster:
	â€¢	FastAPI (serving your model)
	â€¢	Prometheus (scraping /metrics)
	â€¢	Grafana (showing dashboards)
	â€¢	They all run in separate containers but communicate with each other on the same Docker network

